{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3b8c5a-83c1-41f3-9d46-55a4ac0e21d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\nicol\\appdata\\roaming\\python\\python312\\site-packages (2.9.10)\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\nicol\\appdata\\roaming\\python\\python312\\site-packages (9.4.0)\n",
      "Requirement already satisfied: pyodbc in c:\\programdata\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary mysql-connector-python pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e7ffde-0967-47e5-b9da-43764d1ba146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting oracledb\n",
      "  Downloading oracledb-3.3.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: cryptography>=3.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from oracledb) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=3.2.1->oracledb) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.2.1->oracledb) (2.21)\n",
      "Downloading oracledb-3.3.0-cp312-cp312-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: oracledb\n",
      "Successfully installed oracledb-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install oracledb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8475388a-2625-4078-a378-2e95a79ea7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Load a CSV (same schema) into Oracle, PostgreSQL, MySQL, and SQL Server.\n",
    "\n",
    "- PostgreSQL  : COPY FROM STDIN (fastest)\n",
    "- MySQL       : LOAD DATA LOCAL INFILE (fastest from client)\n",
    "- SQL Server  : BULK INSERT (fastest) with fallback to fast_executemany\n",
    "- Oracle      : Array DML with executemany (fast via Python driver)\n",
    "\n",
    "CSV must have header:\n",
    "Block_num,Trx_id,Timestamp,Amount,Contract_type,Currency,Event_name,From_Address,To_Address\n",
    "\n",
    "Usage examples:\n",
    "  python load_csv_to_rdbms.py --create        # create tables then load all\n",
    "  python load_csv_to_rdbms.py --only pg,mysql # load a subset\n",
    "  python load_csv_to_rdbms.py --rows 10000    # test with first 10k rows only\n",
    "\n",
    "Edit CONFIG below to point to your CSV file paths and DSNs.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "CONFIG = {\n",
    "    \"csvs\": {\n",
    "        # point each DB to its CSV on your server\n",
    "        \"oracle\":   Path(\"//wsl.localhost/Ubuntu/home/nicolas/multi-db-env/data/block_transactions_oracle.csv\"),\n",
    "        \"postgres\": Path(\"//wsl.localhost/Ubuntu/home/nicolas/multi-db-env/data/block_transactions_postgres.csv\"),\n",
    "        \"mysql\":    Path(\"//wsl.localhost/Ubuntu/home/nicolas/multi-db-env/data/block_transactions_mysql.csv\"),\n",
    "        \"mssql\":    Path(\"//wsl.localhost/Ubuntu/home/nicolas/multi-db-env/data/block_transactions_mssql.csv\") if os.name == \"nt\"\n",
    "                    else Path(\"/wsl.localhost/Ubuntu/home/nicolas/multi-db-env/data/block_transactions_mssql.csv\"),\n",
    "    },\n",
    "    \"oracle\": {\n",
    "        # supports cx_Oracle or python-oracledb (thin or thick)\n",
    "        \"dsn\": {\"user\": \"system\", \"password\": \"oracle\", \"dsn\": \"localhost:1521/XEPDB1\"},\n",
    "        \"table\": \"block_transactions\",\n",
    "    },\n",
    "    \"postgres\": {\n",
    "        \"dsn\": \"dbname=testdb user=admin password=admin host=127.0.0.1 port=5434\",\n",
    "        \"table\": \"block_transactions\",\n",
    "        \"schema\": None,  # or \"public\"\n",
    "    },\n",
    "    \"mysql\": {\n",
    "        \"dsn\": {\n",
    "            \"user\": \"root\",\n",
    "            \"password\": \"root\",\n",
    "            \"host\": \"127.0.0.1\",\n",
    "            \"database\": \"testdb\",\n",
    "            \"port\": 3307,\n",
    "            \"allow_local_infile\": True,\n",
    "        },\n",
    "        \"table\": \"block_transactions\",\n",
    "    },\n",
    "    \"mssql\": {\n",
    "        \"dsn\": \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=master;UID=sa;PWD=Admin@12345\",\n",
    "        \"table\": \"block_transactions\",\n",
    "    },\n",
    "}\n",
    "\n",
    "COLUMNS = [\n",
    "    \"Block_num\",\"Trx_id\",\"Timestamp\",\"Amount\",\"Contract_type\",\n",
    "    \"Currency\",\"Event_name\",\"From_Address\",\"To_Address\"\n",
    "]\n",
    "\n",
    "# -------------------- UTIL --------------------\n",
    "def head_count(path: Path, limit_rows: int | None) -> int:\n",
    "    if limit_rows:\n",
    "        return limit_rows\n",
    "    # count lines quickly (minus header)\n",
    "    with path.open(\"rb\") as f:\n",
    "        return max(0, sum(1 for _ in f) - 1)\n",
    "\n",
    "def ensure_exists(path: Path, label: str):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{label} CSV not found at: {path}\")\n",
    "\n",
    "# -------------------- CREATE TABLES --------------------\n",
    "def create_oracle():\n",
    "    try:\n",
    "        import oracledb as cx_oracle\n",
    "    except Exception:\n",
    "        import cx_Oracle as cx_oracle  # fallback\n",
    "    conn = cx_oracle.connect(**CONFIG[\"oracle\"][\"dsn\"])\n",
    "    cur = conn.cursor()\n",
    "    tbl = CONFIG[\"oracle\"][\"table\"]\n",
    "    cur.execute(\"\"\"\n",
    "    BEGIN\n",
    "      EXECUTE IMMEDIATE 'DROP TABLE block_transactions PURGE';\n",
    "    EXCEPTION WHEN OTHERS THEN\n",
    "      IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "    END;\"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE {tbl} (\n",
    "          Block_num      NUMBER(10),\n",
    "          Trx_id         VARCHAR2(100),\n",
    "          \"Timestamp\"    TIMESTAMP,\n",
    "          Amount         NUMBER(20,8),\n",
    "          Contract_type  VARCHAR2(50),\n",
    "          Currency       VARCHAR2(10),\n",
    "          Event_name     VARCHAR2(100),\n",
    "          From_Address   VARCHAR2(100),\n",
    "          To_Address     VARCHAR2(100)\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close(); conn.close()\n",
    "\n",
    "def create_postgres():\n",
    "    import psycopg2\n",
    "    dsn = CONFIG[\"postgres\"][\"dsn\"]\n",
    "    tbl = CONFIG[\"postgres\"][\"table\"]\n",
    "    schema = CONFIG[\"postgres\"][\"schema\"]\n",
    "    qname = f'\"{tbl}\"' if not schema else f'\"{schema}\".\"{tbl}\"'\n",
    "    conn = psycopg2.connect(dsn)\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f'DROP TABLE IF EXISTS {qname}')\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE {qname} (\n",
    "          \"Block_num\"      INTEGER,\n",
    "          \"Trx_id\"         VARCHAR(100),\n",
    "          \"Timestamp\"      TIMESTAMPTZ,\n",
    "          \"Amount\"         NUMERIC(20,8),\n",
    "          \"Contract_type\"  VARCHAR(50),\n",
    "          \"Currency\"       VARCHAR(10),\n",
    "          \"Event_name\"     VARCHAR(100),\n",
    "          \"From_Address\"   VARCHAR(100),\n",
    "          \"To_Address\"     VARCHAR(100)\n",
    "        )\n",
    "    \"\"\")\n",
    "    cur.close(); conn.close()\n",
    "\n",
    "def create_mysql():\n",
    "    import mysql.connector as mysql\n",
    "    conn = mysql.connect(**CONFIG[\"mysql\"][\"dsn\"])\n",
    "    cur = conn.cursor()\n",
    "    tbl = CONFIG[\"mysql\"][\"table\"]\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS `{tbl}`\")\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE `{tbl}`(\n",
    "          `Block_num`     INT,\n",
    "          `Trx_id`        VARCHAR(100),\n",
    "          `Timestamp`     TIMESTAMP NULL,\n",
    "          `Amount`        DECIMAL(20,8),\n",
    "          `Contract_type` VARCHAR(50),\n",
    "          `Currency`      VARCHAR(10),\n",
    "          `Event_name`    VARCHAR(100),\n",
    "          `From_Address`  VARCHAR(100),\n",
    "          `To_Address`    VARCHAR(100)\n",
    "        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close(); conn.close()\n",
    "\n",
    "def create_mssql():\n",
    "    import pyodbc\n",
    "    cn = pyodbc.connect(CONFIG[\"mssql\"][\"dsn\"], autocommit=True)\n",
    "    cur = cn.cursor()\n",
    "    tbl = CONFIG[\"mssql\"][\"table\"]\n",
    "    cur.execute(f\"IF OBJECT_ID(N'[dbo].[{tbl}]', N'U') IS NOT NULL DROP TABLE [dbo].[{tbl}]\")\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE [dbo].[{tbl}](\n",
    "          [Block_num]     INT,\n",
    "          [Trx_id]        VARCHAR(100),\n",
    "          [Timestamp]     DATETIME2,\n",
    "          [Amount]        DECIMAL(20,8),\n",
    "          [Contract_type] VARCHAR(50),\n",
    "          [Currency]      VARCHAR(10),\n",
    "          [Event_name]    VARCHAR(100),\n",
    "          [From_Address]  VARCHAR(100),\n",
    "          [To_Address]    VARCHAR(100)\n",
    "        )\n",
    "    \"\"\")\n",
    "    cur.close(); cn.close()\n",
    "\n",
    "# -------------------- LOADERS --------------------\n",
    "def load_oracle(limit_rows: int | None):\n",
    "    # executemany with array binding\n",
    "    try:\n",
    "        import oracledb as cx_oracle\n",
    "    except Exception:\n",
    "        import cx_Oracle as cx_oracle\n",
    "    path = CONFIG[\"csvs\"][\"oracle\"]; ensure_exists(path, \"Oracle\")\n",
    "    rows_target = head_count(path, limit_rows)\n",
    "\n",
    "    conn = cx_oracle.connect(**CONFIG[\"oracle\"][\"dsn\"])\n",
    "    cur = conn.cursor()\n",
    "    # help timestamp parsing consistency\n",
    "    try:\n",
    "        cur.execute(\"ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD\\\"T\\\"HH24:MI:SS\\\"Z\\\"'\")\n",
    "        cur.execute(\"ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD\\\"T\\\"HH24:MI:SS'\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    sql = f\"\"\"INSERT /*+ APPEND */ INTO {CONFIG[\"oracle\"][\"table\"]}\n",
    "        (Block_num, Trx_id, \"Timestamp\", Amount, Contract_type, Currency, Event_name, From_Address, To_Address)\n",
    "        VALUES (:1,:2,:3,:4,:5,:6,:7,:8,:9)\"\"\"\n",
    "\n",
    "    batch, batch_size, count = [], 10_000, 0\n",
    "    t0 = time.time()\n",
    "    with path.open(newline=\"\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            if limit_rows and count >= limit_rows:\n",
    "                break\n",
    "            # strip trailing Z for Oracle TIMESTAMP (no TZ)\n",
    "            ts = row[\"Timestamp\"].rstrip(\"Z\")\n",
    "            batch.append((\n",
    "                int(row[\"Block_num\"]),\n",
    "                row[\"Trx_id\"],\n",
    "                datetime.fromisoformat(ts),\n",
    "                float(row[\"Amount\"]),\n",
    "                row[\"Contract_type\"],\n",
    "                row[\"Currency\"],\n",
    "                row[\"Event_name\"],\n",
    "                row[\"From_Address\"],\n",
    "                row[\"To_Address\"]\n",
    "            ))\n",
    "            count += 1\n",
    "            if len(batch) >= batch_size:\n",
    "                cur.executemany(sql, batch); batch.clear()\n",
    "        if batch:\n",
    "            cur.executemany(sql, batch)\n",
    "    conn.commit()\n",
    "    cur.close(); conn.close()\n",
    "    print(f\"[Oracle] Loaded {count:,} rows in {time.time()-t0:,.1f}s\")\n",
    "\n",
    "def load_postgres(limit_rows: int | None):\n",
    "    import psycopg2\n",
    "    path = CONFIG[\"csvs\"][\"postgres\"]; ensure_exists(path, \"PostgreSQL\")\n",
    "    dsn = CONFIG[\"postgres\"][\"dsn\"]; tbl = CONFIG[\"postgres\"][\"table\"]; schema = CONFIG[\"postgres\"][\"schema\"]\n",
    "    qname = f'\"{tbl}\"' if not schema else f'\"{schema}\".\"{tbl}\"'\n",
    "    rows_target = head_count(path, limit_rows)\n",
    "\n",
    "    conn = psycopg2.connect(dsn)\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    t0 = time.time()\n",
    "\n",
    "    if limit_rows:\n",
    "        # copy N rows manually (simple & reliable for testing)\n",
    "        with path.open(newline=\"\") as f:\n",
    "            rdr = csv.reader(f)\n",
    "            header = next(rdr)\n",
    "            # write to temp file with only N lines\n",
    "            tmp = Path(path.parent, f\"tmp_{tbl}_{limit_rows}.csv\")\n",
    "            with tmp.open(\"w\", newline=\"\") as out:\n",
    "                w = csv.writer(out); w.writerow(header)\n",
    "                for i, row in enumerate(rdr, start=1):\n",
    "                    if i > limit_rows: break\n",
    "                    w.writerow(row)\n",
    "            with tmp.open(\"r\") as tf:\n",
    "                copy_sql = f'COPY {qname} (\"' + '\",\"'.join(COLUMNS) + '\") FROM STDIN WITH (FORMAT csv, HEADER true)'\n",
    "cur.copy_expert(copy_sql, tf)\n",
    "            tmp.unlink(missing_ok=True)\n",
    "    else:\n",
    "        with path.open(\"r\") as f:\n",
    "            copy_sql = f'COPY {qname} (\"' + '\",\"'.join(COLUMNS) + '\") FROM STDIN WITH (FORMAT csv, HEADER true)'\n",
    "            cur.copy_expert(copy_sql, f)\n",
    "\n",
    "    print(f\"[PostgreSQL] Loaded {rows_target:,} rows in {time.time()-t0:,.1f}s\")\n",
    "    cur.close(); conn.close()\n",
    "\n",
    "def load_mysql(limit_rows: int | None):\n",
    "    import mysql.connector as mysql\n",
    "    path = CONFIG[\"csvs\"][\"mysql\"]; ensure_exists(path, \"MySQL\")\n",
    "    conn = mysql.connect(**CONFIG[\"mysql\"][\"dsn\"])\n",
    "    cur = conn.cursor()\n",
    "    # allow LOCAL\n",
    "    try:\n",
    "        cur.execute(\"SET GLOBAL local_infile = 1\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    cur.execute(\"SET SESSION sql_mode=(SELECT REPLACE(@@sql_mode,'STRICT_TRANS_TABLES',''))\")\n",
    "\n",
    "    tbl = CONFIG[\"mysql\"][\"table\"]\n",
    "    rows_target = head_count(path, limit_rows)\n",
    "    t0 = time.time()\n",
    "\n",
    "    if limit_rows:\n",
    "        # write temp N-row CSV\n",
    "        with path.open(newline=\"\") as f:\n",
    "            rdr = csv.reader(f)\n",
    "            header = next(rdr)\n",
    "            tmp = Path(path.parent, f\"tmp_{tbl}_{limit_rows}.csv\")\n",
    "            with tmp.open(\"w\", newline=\"\") as out:\n",
    "                w = csv.writer(out); w.writerow(header)\n",
    "                for i, row in enumerate(rdr, start=1):\n",
    "                    if i > limit_rows: break\n",
    "                    w.writerow(row)\n",
    "        load_path = tmp.as_posix()\n",
    "    else:\n",
    "        load_path = path.as_posix()\n",
    "\n",
    "    load_sql = f\"\"\"\n",
    "        LOAD DATA LOCAL INFILE '{load_path}'\n",
    "        INTO TABLE `{tbl}`\n",
    "        FIELDS TERMINATED BY ','\n",
    "        ENCLOSED BY '\"'\n",
    "        LINES TERMINATED BY '\\\\n'\n",
    "        IGNORE 1 LINES\n",
    "        (`Block_num`,`Trx_id`,`Timestamp`,`Amount`,`Contract_type`,`Currency`,`Event_name`,`From_Address`,`To_Address`);\n",
    "    \"\"\"\n",
    "    cur.execute(load_sql)\n",
    "    conn.commit()\n",
    "    if limit_rows:\n",
    "        Path(load_path).unlink(missing_ok=True)\n",
    "    print(f\"[MySQL]  Loaded {rows_target:,} rows in {time.time()-t0:,.1f}s\")\n",
    "    cur.close(); conn.close()\n",
    "\n",
    "def load_mssql(limit_rows: int | None):\n",
    "    import pyodbc, csv as _csv\n",
    "    path = CONFIG[\"csvs\"][\"mssql\"]; ensure_exists(path, \"SQL Server\")\n",
    "    tbl = CONFIG[\"mssql\"][\"table\"]\n",
    "    rows_target = head_count(path, limit_rows)\n",
    "\n",
    "    # Prefer BULK INSERT (server must read path). If not, fallback fast_executemany.\n",
    "    cn = pyodbc.connect(CONFIG[\"mssql\"][\"dsn\"], autocommit=True)\n",
    "    cur = cn.cursor()\n",
    "    t0 = time.time()\n",
    "    bulk_path = str(path)\n",
    "\n",
    "    if limit_rows:\n",
    "        # create temp file with N rows\n",
    "        with path.open(newline=\"\") as f:\n",
    "            rdr = _csv.reader(f)\n",
    "            header = next(rdr)\n",
    "            tmp = Path(path.parent, f\"tmp_{tbl}_{limit_rows}.csv\")\n",
    "            with tmp.open(\"w\", newline=\"\") as out:\n",
    "                w = _csv.writer(out); w.writerow(header)\n",
    "                for i, row in enumerate(rdr, start=1):\n",
    "                    if i > limit_rows: break\n",
    "                    w.writerow(row)\n",
    "        bulk_path = str(tmp)\n",
    "\n",
    "    try:\n",
    "        cur.execute(f\"\"\"\n",
    "            BULK INSERT [{tbl}]\n",
    "            FROM '{bulk_path}'\n",
    "            WITH (\n",
    "                FIRSTROW = 2,\n",
    "                FIELDTERMINATOR = ',',\n",
    "                ROWTERMINATOR = '\\\\n',\n",
    "                FORMAT = 'CSV',\n",
    "                TABLOCK,\n",
    "                KEEPNULLS,\n",
    "                CODEPAGE = 'RAW'\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(f\"[MSSQL]  BULK INSERT loaded {rows_target:,} rows in {time.time()-t0:,.1f}s\")\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"[MSSQL]  BULK INSERT failed: {e}. Falling back to fast_executemany…\")\n",
    "        cn2 = pyodbc.connect(CONFIG[\"mssql\"][\"dsn\"]); cn2.autocommit = False\n",
    "        cur2 = cn2.cursor(); cur2.fast_executemany = True\n",
    "        sql = f\"\"\"INSERT INTO [{tbl}] ([Block_num],[Trx_id],[Timestamp],[Amount],[Contract_type],[Currency],[Event_name],[From_Address],[To_Address])\n",
    "                  VALUES (?,?,?,?,?,?,?,?,?)\"\"\"\n",
    "        batch, batch_size, count = [], 10_000, 0\n",
    "        with path.open(newline=\"\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                if limit_rows and count >= limit_rows: break\n",
    "                batch.append([\n",
    "                    int(row[\"Block_num\"]),\n",
    "                    row[\"Trx_id\"],\n",
    "                    row[\"Timestamp\"].rstrip(\"Z\"),\n",
    "                    float(row[\"Amount\"]),\n",
    "                    row[\"Contract_type\"],\n",
    "                    row[\"Currency\"],\n",
    "                    row[\"Event_name\"],\n",
    "                    row[\"From_Address\"],\n",
    "                    row[\"To_Address\"],\n",
    "                ])\n",
    "                count += 1\n",
    "                if len(batch) >= batch_size:\n",
    "                    cur2.executemany(sql, batch); cn2.commit(); batch.clear()\n",
    "            if batch:\n",
    "                cur2.executemany(sql, batch); cn2.commit()\n",
    "        cur2.close(); cn2.close()\n",
    "        print(f\"[MSSQL]  fast_executemany loaded {count:,} rows in {time.time()-t0:,.1f}s\")\n",
    "    finally:\n",
    "        if limit_rows and 'tmp' in locals():\n",
    "            Path(bulk_path).unlink(missing_ok=True)\n",
    "        cur.close(); cn.close()\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Load CSVs into Oracle/PostgreSQL/MySQL/MSSQL.\")\n",
    "    p.add_argument(\"--create\", action=\"store_true\", help=\"Create/replace tables before loading.\")\n",
    "    p.add_argument(\"--only\", type=str, default=\"oracle,postgres,mysql,mssql\",\n",
    "                   help=\"Comma list of targets to run (oracle,postgres,mysql,mssql).\")\n",
    "    p.add_argument(\"--rows\", type=int, default=None, help=\"Limit rows per DB (for testing).\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    targets = [t.strip().lower() for t in args.only.split(\",\") if t.strip()]\n",
    "\n",
    "    for t in targets:\n",
    "        if t not in (\"oracle\",\"postgres\",\"mysql\",\"mssql\"):\n",
    "            print(f\"Unknown target '{t}'. Allowed: oracle, postgres, mysql, mssql\")\n",
    "            sys.exit(2)\n",
    "\n",
    "    # CREATE\n",
    "    if args.create:\n",
    "        if \"oracle\" in targets:   print(\"[Oracle]  creating table…\");   create_oracle()\n",
    "        if \"postgres\" in targets: print(\"[Postgres] creating table…\");  create_postgres()\n",
    "        if \"mysql\" in targets:    print(\"[MySQL]   creating table…\");   create_mysql()\n",
    "        if \"mssql\" in targets:    print(\"[MSSQL]   creating table…\");   create_mssql()\n",
    "\n",
    "    # LOAD\n",
    "    if \"oracle\" in targets:   load_oracle(args.rows)\n",
    "    if \"postgres\" in targets: load_postgres(args.rows)\n",
    "    if \"mysql\" in targets:    load_mysql(args.rows)\n",
    "    if \"mssql\" in targets:    load_mssql(args.rows)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc4524-f9bf-4ac2-8588-1066f8e8a1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
